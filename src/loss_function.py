import pandas as pd
import statsmodels.api as sm
import h5py
import numpy as np
from rich import print
from scipy.optimize import linear_sum_assignment, minimize
import solver

cache = {}  # For speedup during mulitiple ED during model optimization


def descriptor_distance(
    ai_df: pd.DataFrame, model_descriptors: dict, matches: list[str], norm
) -> np.array:  # all descriptors are normalized together from 0 to 1
    """Calculates the distance of the descriptors after normalizing, given the list of matching parameters.
    Formula:

    Parameters
    ----------
    ai_df : pd.DataFrame
        The descriptor information of the ab initio data. The keys should match model descriptors'.
    model_descriptors : dict
        The descriptor information of the model data. Normally is generated by solver.py.
    matches : list[str]
        List of descriptor names as strings
    norm : float
        Norm to normalize the incoming data. (sigma_{E}_{ab})^2

    Returns
    -------
    np.array
        nstates_ai by nstates_model estimation of the distance
    """
    nai = len(ai_df)
    nmodel = model_descriptors[matches[0]].shape[0]
    distance = np.zeros((nai, nmodel))

    for k in matches:
        xai = ai_df[k].values
        xmodel = model_descriptors[k]

        distance += (xai[:, np.newaxis] - xmodel[np.newaxis, :]) ** 2 / norm[k]  # euclidean

    distance = distance / len(matches)
    return distance


def unmapped_penalty(energy_m_unmapped, max_energy_ab: float, norm):
    """Fills out the rest of the distance matrix. The penalty matrix to map unmapped to ab initio states.

    Parameters
    ----------
    energy_m_unmapped : nd.array(float)
        The energies of the model states that are unmapped.
    max_energy_ab : float
        Maximum ab initio energy in the dataset.
    norm : float
        Norm to normalize the incoming data. (sigma_{E}_{ab})^2

    Returns
    -------
    nd.array
        Shape: (len(energy_m_unmapped), nroots for model). Penalty for the unmapped states to be mapped to
        ab initio states.
    """
    penalty = np.sum(
        (np.maximum(0, max_energy_ab - energy_m_unmapped) ** 2) / norm["energy"]
    )
    return penalty


def give_boltzmann_weights(energy_ab, e_0, beta):
    """Return the boltzmann weights given a beta

    Parameters
    ----------
    energy_ab : nd.array(float)
        Length of ab initio states, the energy of each state.
    e_0 : float
        The ground state energy
    beta : float
        Boltmann beta, sets the expentional decay of the state weighting.

    Returns
    -------
    nd.array(float)
        boltztman weights, the ground state always get 1. lower energy states get more weighting then exponentially
        decays.
    """
    # give boltzmann weights given the boltzmann factor
    # beta = 1/(k_b * T)

    boltzmann_weights = np.exp(-beta * (energy_ab - e_0))
    return boltzmann_weights


def evaluate_loss(
    params: np.ndarray,
    keys,
    weights,
    boltzmann_weights,
    onebody,
    twobody,
    ai_df,
    max_ai_energy,
    nroots,
    matches,
    fcivec,
    norm,
) -> float:
    """The actual Loss functional.

    Parameters
    ----------
    params : np.ndarray
        Parameters of the hamiltonian. The keys should match the key names of the onebody and twobody operators.
    keys : List
        Used to make params a pd.Series. keys should match the key names of the onebody and twobody operators.
    weights : List
        w_0, w_1
    boltzmann_weights : nd.array(float)
        boltztman weights, the ground state always get 1. lower energy states get more weighting then exponentially
        decays.
    onebody : dict
        (keys are strings, values are 2D np.arrays) tij. Dictionary of the 1-body operators. Each operator of the
        shape (norb,norb).
    twobody : dict
        (keys are strings, values are 4D np.arrays) Vijkl. Dictionary of the 2-body operators. Each operator of the
        shape (norb,norb,norb,norb).
    ai_df : pd.DataFrame
        The descriptor information of the ab initio data. The keys should match model descriptors'.
    max_ai_energy : float
        Maximum of the whole ab initio data. See np.max(ai_df["energy"]) in mapping.
    nroots : int
        Number of eigenstates to solve the model Hamiltonian for.
    matches : list[str]
        Which descriptors to use for the descriptor distance calculation.
    fcivec : nd.array()
        Stored fcivecs to pass into the solver, so that full model diagonalization does not take as long during
        optimization.
    norm : float
        Norm to normalize the incoming data. (sigma_{E}_{ab})^2

    Returns
    -------
    float
        Loss function value.
    """
    w_0 = weights[0]
    w_1 = weights[1]

    if fcivec is None and "fcivec" in cache:
        fcivec = cache["fcivec"]

    params = pd.Series(params, index=keys)

    descriptors, fcivec = solver.solve_effective_hamiltonian(
        onebody, twobody, params, nroots=nroots, ci0=fcivec
    )
    cache["fcivec"] = fcivec

    dist_des = descriptor_distance(
        ai_df, descriptors, matches=matches, norm=norm
    )
    dist_energy = descriptor_distance(
        ai_df, descriptors, matches=["energy"], norm=norm
    )
    #distance = dist_energy + dist_des
    #row_ind, col_ind = linear_sum_assignment(distance)

    #sloss = np.sum(boltzmann_weights * dist_energy[row_ind, col_ind])
    #dloss = np.sum(boltzmann_weights * dist_des[row_ind, col_ind])
    #not_col_ind = np.delete(np.arange(nroots), col_ind)

    # Debugging information to test penalty
    #print(row_ind)
    #print(col_ind)
    #print()
    #print(not_col_ind)
    #print(descriptors["energy"])

    #penalty = unmapped_penalty(
    #    descriptors["energy"][not_col_ind], max_ai_energy, norm=norm
    #)

    distance = w_0 * dist_energy + w_1 * dist_des

    penalty = np.maximum(0, max_ai_energy - descriptors['energy'])

    npenalty = nroots - len(ai_df)
    distance = np.vstack((distance, np.tile(w_0 * penalty, (npenalty, 1))))  # replace 6 with nroots-len(ai_df)

    row_ind, col_ind = linear_sum_assignment(distance)

    loss = np.sum(distance[row_ind, col_ind])

    # Debugging information to test loss
    #print()
    #print("loss ", loss)
    #print("params", params.values)

    return {
        "loss": loss,
        "sloss": np.sum(dist_energy[row_ind[:len(ai_df)], col_ind[:len(ai_df)]]),
        "dloss": np.sum(dist_des[row_ind[:len(ai_df)], col_ind[:len(ai_df)]]),
        "penalty": np.sum(np.tile(penalty, (npenalty, 1))[row_ind[len(ai_df):] - len(ai_df), col_ind[len(ai_df):]]), # NEEDS TO BE FIXED
        "descriptors": descriptors,
        "distance": distance,
        "row_ind": row_ind,
        "col_ind": col_ind,
        "params": params,
    }


# maps for train states first, then maps the rest of the model states to test states.
def CV_evaluate_loss(
    params: np.ndarray,
    keys,
    weights,
    onebody,
    twobody,
    ai_df,
    max_ai_energy,
    nroots,
    matches,
    fcivec,
    norm,
    train_states,
    test_states,
) -> float:

    w_0 = weights[0]
    w_1 = weights[1]

    if fcivec is None and "fcivec" in cache:
        fcivec = cache["fcivec"]

    params = pd.Series(params, index=keys)

    descriptors, fcivec = solver.solve_effective_hamiltonian(
        onebody, twobody, params, nroots=nroots, ci0=None
    )
    cache["fcivec"] = fcivec

    dist_des = descriptor_distance(ai_df, descriptors, matches=matches, norm=norm)
    dist_energy = descriptor_distance(ai_df, descriptors, matches=["energy"], norm=norm)

    distance = w_0 * dist_energy + w_1 * dist_des

    penalty = np.maximum(0, max_ai_energy - descriptors['energy'])

    npenalty = nroots - len(ai_df)
    distance_train = np.vstack((distance[train_states], np.tile(w_0 * penalty, (npenalty, 1))))
    distance = np.vstack((distance, np.tile(w_0 * penalty, (npenalty, 1))))

    row_ind, col_ind = linear_sum_assignment(distance_train)
    for p in np.sort(test_states):
        ind = np.where(row_ind == p)[0][0]
        for i in range(ind, len(row_ind)):
            row_ind[i] += 1

    loss = np.sum(distance[row_ind, col_ind])

    # mapping the left over train states with left over model states

    x = np.delete(np.arange(0, nroots), col_ind)
    dist = distance[test_states][:, x]
    rows, cols = linear_sum_assignment(dist)

    test_loss = np.sum(dist[rows, cols])

    train_row_inds = row_ind[np.where(row_ind < len(ai_df))[0]]
    train_col_inds = col_ind[np.where(row_ind < len(ai_df))[0]]
    test_row_inds = test_states[rows]
    test_col_inds = x[cols]
    penalty_row_inds = row_ind[np.where(row_ind >= len(ai_df))[0]]
    penalty_col_inds = col_ind[np.where(row_ind >= len(ai_df))[0]]

    return {
        "train_loss": loss,  # train loss
        "test_loss": test_loss,
        "train_sloss": np.sum(dist_energy[train_row_inds, train_col_inds]),
        "train_dloss": np.sum(dist_des[train_row_inds, train_col_inds]),
        "test_sloss": np.sum(dist_energy[test_row_inds, test_col_inds]),
        "test_dloss": np.sum(dist_des[test_row_inds, test_col_inds]),
        "penalty": np.sum(np.tile(penalty, (npenalty, 1))[:, penalty_col_inds]),
        "descriptors": descriptors,
        "distance": distance,
        "row_ind": row_ind,
        "col_ind": col_ind,
        "params": params,
        'test_rows': np.array(test_states)[rows],
        'test_cols': x[cols]}


def optimize_function(*args, **kwargs):
    return evaluate_loss(*args, **kwargs)["loss"]


def optimize_CV_function(*args, **kwargs):
    return CV_evaluate_loss(*args, **kwargs)["train_loss"]


def mapping(
    onebody: dict,
    twobody: dict,
    onebody_params: list,
    twobody_params: list,
    ai_df: pd.DataFrame,
    nroots: int,
    outfile: str,
    matches: list,
    weights: list,
    beta: float,
    p: int,
    guess_params=None,
):
    """Sets up the loss function to then optimize.

    For nroots, we want to have more than were done in ab initio. The model roots will get mapped onto the ab initio ones, with
    the extra ones dropped. As the number of roots increases, the mapping should get a bit better.

    Parameters
    ----------
    onebody : dict
        (keys are strings, values are 2D np.arrays) tij. Dictionary of the 1-body operators. Each operator of the
        shape (norb,norb).
    twobody : dict
        (keys are strings, values are 4D np.arrays) Vijkl. Dictionary of the 2-body operators. Each operator of the
        shape (norb,norb,norb,norb).
    onebody_params : list
        List of onebody keys which are the parameters to set inside the hamiltonian. list of strings, keys that are
        allowed to have nonzero values in the Hamiltonian
    twobody_params : list
        List of twobody keys which are the parameters to set inside the hamiltonian. list of strings, keys that are
        allowed to have nonzero values in the Hamiltonian
    ai_df : pd.DataFrame
        The descriptor information of the ab initio data. The keys should match model descriptors'. ab initio data, columns are parameter names, and 'energy'
    nroots : int
        Number of eigenstates to solve the model Hamiltonian for. number of roots to solve for in the model
    outfile : str
        File to save the model file to. output to HDF5 file
    matches : list[str]
        List of descriptor names as strings. keys that are used to match the ab initio and model descriptors.
        Should be the descriptors corresponding to the terms in the hamiltonian.
    weights : list
        w_0, w_1. list of floats, [w_0, w_1] weights for the spectrum loss and descriptor loss.
    beta : float
        inverse temperature for the boltzmann weights. 0 means equal weights to all states
    p : int
        Number of states to drop out of optimization for CV.
    guess_params : pd.Series
        Guess parameters to override the DMD parameters if so desired. dict (keys are strings, values are floats)
        overrides the dmd parameters
    """
    p_out_states = np.random.choice(np.arange(0, len(ai_df)), size=p, replace=False)
    #  p_out_states = np.array([p]) # this p is for 30-k_groups, leaving out data 1-by-1
    ai_df_train = ai_df.drop(p_out_states, axis=0)
    ai_df_test = ai_df.loc[p_out_states]
    max_ai_energy = np.max(ai_df["energy"])

    boltzmann_weights_train = give_boltzmann_weights(ai_df_train["energy"], ai_df["energy"][0], beta)
    print(f"Boltzmann weights for beta {beta}: {boltzmann_weights_train}")
    boltzmann_weights_test = give_boltzmann_weights(ai_df_test["energy"], ai_df["energy"][0], beta)
    print(f"Boltzmann weights for beta {beta}: {boltzmann_weights_test}")

    # Starting parameter guess, not sure if this should stay the same despite the p split
    dmd = sm.OLS(ai_df["energy"], ai_df[onebody_params + twobody_params]).fit()
    print(dmd.summary())

    params = dmd.params.copy()

    if guess_params is not None:
        for k in guess_params.keys():
            params[k] = guess_params[k]

    print("Starting Parameters: ", params)

    norm = 2 * np.var(ai_df)  # Should this be based on the ai_df_train? I think not.

    keys = params.keys()
    x0 = params.values

    # OPTMIZATION LOOP START
    print("Starting optimization")

    """ xmin = minimize(
        optimize_function,
        x0,
        args=(
            keys,
            weights,
            boltzmann_weights_train,
            onebody,
            twobody,
            ai_df_train,
            max_ai_energy,
            nroots,
            matches,
            None,
            norm,
        ),
        jac="3-point",
        method="Powell",
        tol=1e-7,
        options={"maxiter": 1000},
    ) """

    xmin = minimize(
        optimize_CV_function,
        x0,
        args=(
            keys,
            weights,
            onebody,
            twobody,
            ai_df,
            max_ai_energy,
            nroots,
            matches,
            None,
            norm,
            np.delete(np.arange(0, len(ai_df)), p_out_states),
            p_out_states,
        ),
        jac="3-point",
        method="Powell",
        tol=1e-7,
        options={"maxiter": 1000},
    )

    print(xmin.nit, xmin.nfev)
    print(xmin.message)
    print("function value", xmin.fun)
    print("parameters", xmin.x)

    print("Evaluate train data after optimization:")
    data = CV_evaluate_loss(
        xmin.x,
        keys,
        weights,
        onebody,
        twobody,
        ai_df,
        max_ai_energy,
        nroots,
        matches,
        None,
        norm,
        np.delete(np.arange(0, len(ai_df)), p_out_states),
        p_out_states,
    )

    N = len(ai_df)
    print("train_loss:", data['train_loss'])
    print("test_loss:", data['test_loss'])
    print("penalty:", data['penalty'])
    print("Ending Params:", data['params'])

    print("train_sloss per state:", data['train_sloss']/N)
    print("test_sloss per state:", data['test_sloss']/N)
    print("train_dloss per state:", data['train_dloss']/N)
    print("test_dloss per state:", data['test_dloss']/N)

    with h5py.File(outfile, "w") as f:
        for k in onebody_params + twobody_params:
            f["dmd_params/" + k] = dmd.params[k]
        f["para_w_0"] = weights[0]
        f["para_w_1"] = weights[1]
        f["loss_loss"] = xmin.fun
        f["ai_spectrum_range (Ha)"] = np.max(ai_df["energy"]) - np.min(ai_df["energy"])

        f["nstates_train"] = len(ai_df_train)
        f["nstates_test"] = len(ai_df_test)
        f["state_ind_for_test"] = p_out_states

        for k in data:
            if k == "descriptors":
                for kk in data[k]:
                    f["train/" + k + "/" + kk] = data[k][kk]
            elif k == "params":
                for i, kk in enumerate(onebody_params + twobody_params):
                    f["rdmd_params/" + kk] = data[k][i]
            else:
                f[k] = data[k]

        """ for k in data_train:
            if k == "descriptors":
                for kk in data_train[k]:
                    f["train/" + k + "/" + kk] = data_train[k][kk]
            elif k == "params":
                for i, kk in enumerate(onebody_params + twobody_params):
                    f["train/" + "rdmd_params/" + kk] = data_train[k][i]
            else:
                f["train/" + k] = data_train[k]

        for k in data_test:
            if k == "descriptors":
                for kk in data_test[k]:
                    f["test/" + k + "/" + kk] = data_test[k][kk]
            elif k == "params":
                for i, kk in enumerate(onebody_params + twobody_params):
                    f["test/" + "rdmd_params/" + kk] = data_test[k][i]
            else:
                f["test/" + k] = data_test[k] """

        f["iterations"] = xmin.nit
        f["termination_message"] = xmin.message
